{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import csv\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import  EnglishStemmer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files(dir):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            file_paths.append(os.path.join(root, name))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files('D:\\\\3rd year\\\\2nd sem\\\\20news-bydate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(file):\n",
    "    with open(file,'r') as f:\n",
    "        read_data = f.read()\n",
    "    read_data.encode('utf-8', 'ignore')\n",
    "    return read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catogorizing(file_path):\n",
    "    comp=[]\n",
    "    sci=[]\n",
    "    rec=[]\n",
    "    talk=[] \n",
    "    for file in file_path:\n",
    "        if \"comp\" in file:\n",
    "            comp.append(read(file))\n",
    "        if \"sci\" in file:\n",
    "            sci.append(read(file))\n",
    "        if \"talk\" in file:\n",
    "            talk.append(read(file))\n",
    "        if \"rec\" in file:\n",
    "            rec.append(read(file))\n",
    "    return comp,sci,talk,rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data,sci_data,talk_data,rec_data=catogorizing(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comp,test_comp = train_test_split(comp_data, train_size=0.7,test_size=0.3)\n",
    "train_rec,test_rec = train_test_split(sci_data, train_size=0.7,test_size=0.3)\n",
    "train_sci,test_sci = train_test_split(talk_data, train_size=0.7,test_size=0.3)\n",
    "train_talk,test_talk = train_test_split(rec_data, train_size=0.7,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[train_comp, train_rec, train_sci, train_talk]\n",
    "list_data=[test_comp, test_rec, test_sci, test_talk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(train_data):\n",
    "    cleaned_list=[]\n",
    "    list_all=[]\n",
    "    for file in train_data:\n",
    "        for document in file:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}')\n",
    "            token_words=tokenizer.tokenize(document)\n",
    "            for word in token_words:\n",
    "                word=word.lower()\n",
    "                word = word.translate(str.maketrans('','',string.punctuation))\n",
    "                stemmer = EnglishStemmer()\n",
    "                word = stemmer.stem(word)\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                list_all.append(word)\n",
    "            for words in list_all:\n",
    "                if words not in stop_words:\n",
    "                    cleaned_list.append(words)\n",
    "                    #print(cleaned_list)\n",
    "    return list(set(cleaned_list))#remove duplicate and append to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=get_unique_words(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(file_list,list_name):\n",
    "    list_words=[]\n",
    "    cleaned_data=[]\n",
    "    for file in file_list:\n",
    "        stemmer = EnglishStemmer()\n",
    "        word = stemmer.stem(file)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word = lemmatizer.lemmatize(file)\n",
    "        list_name.append(file)\n",
    "        #word_list = set([word for word in wn.words(lang='eng')])\n",
    "        #tokenizer = TreebankWordTokenizer() \n",
    "        #detokenizer = TreebankWordDetokenizer()\n",
    "        #cleaned_data += [detokenizer.detokenize([word for word in list_name if word in word_list if word.isalpha()])]\n",
    "    return list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comp_train=[]\n",
    "list_sci_train=[]\n",
    "list_talk_train=[]\n",
    "list_rec_train=[]\n",
    "list_comp_test=[]\n",
    "list_sci_test=[]\n",
    "list_talk_test=[]\n",
    "list_rec_test=[]\n",
    "cleaned_comp_train=cleaning(train_comp,list_comp_train)\n",
    "cleaned_sci_train=cleaning(train_sci,list_sci_train)\n",
    "cleaned_talk_train=cleaning(train_talk,list_talk_train)\n",
    "cleaned_rec_train=cleaning(train_rec,list_rec_train)\n",
    "cleaned_comp_test=cleaning(test_comp,list_comp_test)\n",
    "cleaned_sci_test=cleaning(test_sci,list_sci_test)\n",
    "cleaned_talk_test=cleaning(test_talk,list_talk_test)\n",
    "cleaned_rec_test=cleaning(test_rec,list_rec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_set = get_unique_words(train_data)\n",
    "unique_word_set.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorize_csv(cleaned_data, name, vocabulary_set):\n",
    "    stop_word_set = set(stopwords.words('english') + list(punctuation))\n",
    "    vectorizer = CountVectorizer(stop_words=stop_word_set, vocabulary=vocabulary_set)\n",
    "    x = vectorizer.fit_transform(cleaned_data)\n",
    "    y = vectorizer.get_feature_names()\n",
    "    df = pd.DataFrame(data=x.toarray(), columns=y)\n",
    "    df.to_csv(name+'.csv')    \n",
    "    print(name+'.csv successfully created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp_train.csv successfully created!\n",
      "sci_train.csv successfully created!\n",
      "talk_train.csv successfully created!\n",
      "rec_train.csv successfully created!\n",
      "comp_test.csv successfully created!\n",
      "sci_test.csv successfully created!\n",
      "talk_test.csv successfully created!\n",
      "rec_test.csv successfully created!\n"
     ]
    }
   ],
   "source": [
    "create_vectorize_csv(cleaned_comp_train,'comp_train',unique_word_set)\n",
    "create_vectorize_csv(cleaned_sci_train,'sci_train',unique_word_set)\n",
    "create_vectorize_csv(cleaned_talk_train,'talk_train',unique_word_set)\n",
    "create_vectorize_csv(cleaned_rec_train,'rec_train',unique_word_set)\n",
    "create_vectorize_csv(cleaned_comp_test,'comp_test',unique_word_set)\n",
    "create_vectorize_csv(cleaned_sci_test,'sci_test',unique_word_set)\n",
    "create_vectorize_csv(cleaned_talk_test,'talk_test',unique_word_set)\n",
    "create_vectorize_csv(cleaned_rec_test,'rec_test',unique_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
